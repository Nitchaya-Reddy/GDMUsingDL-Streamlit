{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8-L9PthmC5E9",
        "outputId": "74d7be40-4f30-4288-b630-08346420fde7"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from imblearn.combine import SMOTEENN\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import shap\n",
        "import json\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "# Load the dataset\n",
        "file_path = '/content/Gestational Diabetic Dat Set.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "# Rename the target column for simplicity\n",
        "data.rename(columns={\"Class Label(GDM /Non GDM)\": \"Target\"}, inplace=True)\n",
        "# Drop unnecessary columns\n",
        "data = data.drop(columns=[\"Case Number\"], errors='ignore')\n",
        "# Handle missing values by replacing with column mean\n",
        "data.fillna(data.mean(), inplace=True)\n",
        "# Display dataset summary\n",
        "print(\"Dataset Overview:\")\n",
        "print(data.info())\n",
        "print(\"\\nFirst 5 Rows:\")\n",
        "print(data.head())\n",
        "# Generate visualizations during preprocessing\n",
        "# Histograms for feature distributions\n",
        "\"\"\"This code generates and saves histograms of numerical features in a dataset to visualize their distributions.\n",
        "It adjusts layout for clarity, adds a title, and uses customizable bin sizes, colors, and edges to analyze patterns, outliers, or imbalances.\"\"\"\n",
        "data.hist(figsize=(16, 12), bins=20, color='skyblue', edgecolor='black')\n",
        "plt.suptitle('Feature Distributions', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.savefig(\"feature_distributions.png\")\n",
        "plt.show()\n",
        "# Pair Plot for relationships\n",
        "\"\"\"The code generates a pair plot using Seaborn, visualizing feature relationships and target class distinctions.\n",
        "It uses KDE on the diagonal, distinct colors for each class with hue=\"Target\", and saves the plot as a PNG file for analysis. \"\"\"\n",
        "sns.pairplot(data, hue=\"Target\", palette=\"Set2\", diag_kind=\"kde\")\n",
        "plt.savefig(\"pair_plot.png\")\n",
        "plt.show()\n",
        "# Correlation Heatmap\n",
        "\"\"\"The code creates a correlation heatmap with Seaborn to display the relationships between numerical features in the dataset.\n",
        "It computes the correlation matrix with data.corr(), annotates values on the heatmap, and applies the coolwarm color scheme.\n",
        " The plot is saved as \"correlation_heatmap.png\" for later use.\"\"\"\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = data.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.savefig(\"correlation_heatmap.png\")\n",
        "plt.show()\n",
        "# Box Plots to detect outliers\n",
        "\"\"\"The code generates box plots for numerical features in the dataset to detect outliers.\n",
        "It selects columns with numerical data, creates a box plot for each feature grouped by the \"Target\" variable, and uses the \"Set1\" color palette.\n",
        "Each plot is saved as a PNG file for further review.\"\"\"\n",
        "numerical_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.boxplot(data=data, x=\"Target\", y=feature, palette=\"Set1\")\n",
        "    plt.title(f\"Box Plot of {feature} by Target\")\n",
        "    plt.savefig(f\"box_plot_{feature}.png\")\n",
        "    plt.show()\n",
        "    # Analyze the Target column distribution\n",
        "\"\"\"The code analyzes the distribution of the \"Target\" column, printing the count of each class before applying any resampling techniques like SMOTE-ENN.\n",
        "It uses value_counts() to get the frequency of each class in the \"Target\" column, then prints the counts for Class 0 and Class 1.\n",
        "This helps assess class imbalance in the dataset.\"\"\"\n",
        "print(\"\\nTarget Class Distribution (Before SMOTE-ENN):\")\n",
        "class_counts = data['Target'].value_counts()\n",
        "print(f\"Class 0: {class_counts[0]}\")  # Count of 0s\n",
        "print(f\"Class 1: {class_counts[1]}\")  # Count of 1s\n",
        "# Feature-target split\n",
        "X = data.drop(columns=[\"Target\"])\n",
        "y = data[\"Target\"]\n",
        "# Encode target if it's categorical\n",
        "if y.dtypes == 'object':\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "    # Standardize the features for better model performance\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "# Apply SMOTE-ENN to balance the dataset\n",
        "smote_enn = SMOTEENN(random_state=42)\n",
        "X_smote_enn, y_smote_enn = smote_enn.fit_resample(X, y)\n",
        "# Feature selection\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_smote_enn = selector.fit_transform(X_smote_enn, y_smote_enn)\n",
        "# Analyze the Target column distribution after SMOTE-ENN\n",
        "print(\"\\nTarget Class Distribution (After SMOTE-ENN):\")\n",
        "class_counts_smote_enn = pd.Series(y_smote_enn).value_counts()\n",
        "print(f\"Class 0: {class_counts_smote_enn[0]}\")\n",
        "print(f\"Class 1: {class_counts_smote_enn[1]}\")\n",
        "# Reshape data for Conv1D (samples, time steps, features)\n",
        "X_smote_enn_cnn = X_smote_enn[..., np.newaxis]  # Add a new axis for the feature dimension\n",
        "# Split data into training and testing sets\n",
        "X_train_cnn, X_test_cnn, y_train, y_test = train_test_split(X_smote_enn_cnn, y_smote_enn, test_size=0.2, random_state=42)\n",
        "X_train, X_test, _, _ = train_test_split(X_smote_enn, y_smote_enn, test_size=0.2, random_state=42)\n",
        "# Build the CNN model\n",
        "\"\"\" The code defines a Convolutional Neural Network (CNN) using Keras.\n",
        "It starts with a Conv1D layer (128 filters, kernel size 3) followed by MaxPooling1D to downsample.\n",
        "A second Conv1D layer (64 filters) and pooling layer are applied. The Flatten layer reshapes the output for dense layers.\n",
        "A Dense layer with 128 units and ReLU activation follows, then a Dropout layer (40% rate) helps prevent overfitting.\n",
        " The final Dense layer outputs a single value with a sigmoid activation for binary classification.\n",
        " The model is compiled with Adam optimizer, binary cross-entropy loss, and accuracy as the metric.\n",
        "This structure leverages convolutional layers for feature extraction, pooling for dimensionality reduction, and dense layers for decision-making.\"\"\"\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(128, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Train CNN model\n",
        "cnn_history = cnn_model.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test), epochs=150, batch_size=32, verbose=1)\n",
        "\n",
        "cnn_model.save(\"cnn_model.h5\")\n",
        "\n",
        "# Hyperparameter tuning for SVM\n",
        "\"\"\" The code performs hyperparameter tuning for a Support Vector Machine (SVM) using GridSearchCV.\n",
        "It initializes an SVM model with probability=True and a fixed random_state=42.\n",
        " The hyperparameters to be tuned are C (regularization parameter), kernel (either 'linear' or 'rbf'), and gamma (either 'scale' or 'auto').\n",
        " The grid search uses 5-fold cross-validation (cv=5) and accuracy as the scoring metric.\n",
        "  After fitting the model, the best estimator is retrieved with grid_svm.best_estimator_.\n",
        "This process helps find the optimal hyperparameters for improved model performance.\"\"\"\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "svm_params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
        "grid_svm = GridSearchCV(svm, svm_params, cv=5, scoring='accuracy')\n",
        "grid_svm.fit(X_train, y_train)\n",
        "svm_best = grid_svm.best_estimator_\n",
        "# Train Random Forest model with hyperparameter tuning\n",
        "\"\"\" The code performs hyperparameter tuning for a Random Forest classifier using GridSearchCV.\n",
        "It tunes the n_estimators (number of trees) and max_depth (maximum depth of trees) hyperparameters.\n",
        "The grid search uses 5-fold cross-validation (cv=5) and accuracy as the scoring metric.\n",
        "The best model is obtained via grid_rf.best_estimator_.\"\"\"\n",
        "rf_params = {'n_estimators': [100, 200], 'max_depth': [10, 20, None]}\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy')\n",
        "grid_rf.fit(X_train, y_train)\n",
        "rf_best = grid_rf.best_estimator_\n",
        "# Gradient Boosting Classifier\n",
        "\"\"\" The code defines and trains a Gradient Boosting Classifier model.\n",
        " It initializes the model with n_estimators=100 (number of boosting stages), learning_rate=0.1 (shrinkage factor to control model contribution per stage), and max_depth=3 (maximum depth of individual trees).\n",
        " The random_state=42 ensures reproducibility.\n",
        " After setting the parameters, the model is trained on the training data (X_train, y_train) using the fit() method.\n",
        "Gradient Boosting helps improve predictive performance by combining weak learners iteratively.\"\"\"\n",
        "gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "# Combine models using VotingClassifier\n",
        "\"\"\" The code combines multiple models into an ensemble using VotingClassifier.\n",
        "It includes a Support Vector Machine (svm_best), a Random Forest (rf_best), and a Gradient Boosting Model (gbm_model).\n",
        "The voting='soft' parameter specifies that predictions are made based on the weighted average of predicted probabilities, improving model accuracy.\"\"\"\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[('svm', svm_best), ('rf', rf_best), ('gbm', gbm_model)],\n",
        "    voting='soft'\n",
        ")\n",
        "# Train the ensemble model\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "# Evaluate the ensemble model\n",
        "ensemble_accuracy = ensemble_model.score(X_test, y_test)\n",
        "print(f\"\\nEnsemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
        "# Predictions\n",
        "cnn_pred = (cnn_model.predict(X_test_cnn) > 0.5).astype(\"int32\")\n",
        "svm_pred = svm_best.predict(X_test)\n",
        "rf_pred = rf_best.predict(X_test)\n",
        "gbm_pred = gbm_model.predict(X_test)\n",
        "ensemble_pred = ensemble_model.predict(X_test)\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, ensemble_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig(\"confusion_matrix_ensemble.png\")\n",
        "plt.show()\n",
        "# Classification Report\n",
        "report = classification_report(y_test, ensemble_pred, output_dict=True)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, ensemble_pred))\n",
        "# Save classification report\n",
        "with open(\"classification_report_ensemble.json\", \"w\") as f:\n",
        "    json.dump(report, f)\n",
        "    # ROC-AUC Curve\n",
        "\"\"\" The code generates and plots the ROC-AUC curve for the ensemble model's performance.\n",
        "It first calculates predicted probabilities (ensemble_pred_prob) for the test set, then computes the false positive rate (FPR) and true positive rate (TPR) using roc_curve.\n",
        " The AUC is calculated with auc().\n",
        " The ROC curve is plotted, showing the tradeoff between TPR and FPR, and the plot is saved as \"roc_curve_ensemble.png\".\n",
        "The curve helps evaluate the model's classification ability.\"\"\"\n",
        "# Fit the ensemble model first\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predicted probabilities for the test set\n",
        "ensemble_pred_prob = ensemble_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate FPR, TPR, and AUC for the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, ensemble_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(\"roc_curve_ensemble.png\")\n",
        "plt.show()\n",
        "# Explainability using SHAP\n",
        "\"\"\" The code uses SHAP to explain the Random Forest model (rf_best).\n",
        "It creates a TreeExplainer to compute SHAP values for the test set (X_test) and generates a summary plot to visualize feature importance and contributions to predictions.\"\"\"\n",
        "explainer = shap.TreeExplainer(rf_best)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values, X_test, feature_names=data.columns[:-1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import SMOTEENN\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import shap\n",
        "import json\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "# Load the dataset\n",
        "file_path = 'Gestational Diabetic Dat Set (1)-1.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "# Rename the target column for simplicity\n",
        "data.rename(columns={\"Class Label(GDM /Non GDM)\": \"Target\"}, inplace=True)\n",
        "# Drop unnecessary columns\n",
        "data = data.drop(columns=[\"Case Number\"], errors='ignore')\n",
        "# Handle missing values by replacing with column mean\n",
        "data.fillna(data.mean(), inplace=True)\n",
        "# Display dataset summary\n",
        "print(\"Dataset Overview:\")\n",
        "print(data.info())\n",
        "print(\"\\nFirst 5 Rows:\")\n",
        "print(data.head())\n",
        "# Generate visualizations during preprocessing\n",
        "# Histograms for feature distributions\n",
        "\"\"\"This code generates and saves histograms of numerical features in a dataset to visualize their distributions.\n",
        "It adjusts layout for clarity, adds a title, and uses customizable bin sizes, colors, and edges to analyze patterns, outliers, or imbalances.\"\"\"\n",
        "data.hist(figsize=(16, 12), bins=20, color='skyblue', edgecolor='black')\n",
        "plt.suptitle('Feature Distributions', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.savefig(\"feature_distributions.png\")\n",
        "plt.show()\n",
        "# Pair Plot for relationships\n",
        "\"\"\"The code generates a pair plot using Seaborn, visualizing feature relationships and target class distinctions.\n",
        "It uses KDE on the diagonal, distinct colors for each class with hue=\"Target\", and saves the plot as a PNG file for analysis. \"\"\"\n",
        "sns.pairplot(data, hue=\"Target\", palette=\"Set2\", diag_kind=\"kde\")\n",
        "plt.savefig(\"pair_plot.png\")\n",
        "plt.show()\n",
        "# Correlation Heatmap\n",
        "\"\"\"The code creates a correlation heatmap with Seaborn to display the relationships between numerical features in the dataset.\n",
        "It computes the correlation matrix with data.corr(), annotates values on the heatmap, and applies the coolwarm color scheme.\n",
        " The plot is saved as \"correlation_heatmap.png\" for later use.\"\"\"\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = data.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.savefig(\"correlation_heatmap.png\")\n",
        "plt.show()\n",
        "# Box Plots to detect outliers\n",
        "\"\"\"The code generates box plots for numerical features in the dataset to detect outliers.\n",
        "It selects columns with numerical data, creates a box plot for each feature grouped by the \"Target\" variable, and uses the \"Set1\" color palette.\n",
        "Each plot is saved as a PNG file for further review.\"\"\"\n",
        "numerical_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.boxplot(data=data, x=\"Target\", y=feature, palette=\"Set1\")\n",
        "    plt.title(f\"Box Plot of {feature} by Target\")\n",
        "    plt.savefig(f\"box_plot_{feature}.png\")\n",
        "    plt.show()\n",
        "    # Analyze the Target column distribution\n",
        "\"\"\"The code analyzes the distribution of the \"Target\" column, printing the count of each class before applying any resampling techniques like SMOTE-ENN.\n",
        "It uses value_counts() to get the frequency of each class in the \"Target\" column, then prints the counts for Class 0 and Class 1.\n",
        "This helps assess class imbalance in the dataset.\"\"\"\n",
        "print(\"\\nTarget Class Distribution (Before SMOTE-ENN):\")\n",
        "class_counts = data['Target'].value_counts()\n",
        "print(f\"Class 0: {class_counts[0]}\")  # Count of 0s\n",
        "print(f\"Class 1: {class_counts[1]}\")  # Count of 1s\n",
        "# Feature-target split\n",
        "X = data.drop(columns=[\"Target\"])\n",
        "y = data[\"Target\"]\n",
        "# Encode target if it's categorical\n",
        "if y.dtypes == 'object':\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "    # Standardize the features for better model performance\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "# Apply SMOTE-ENN to balance the dataset\n",
        "smote_enn = SMOTEENN(random_state=42)\n",
        "X_smote_enn, y_smote_enn = smote_enn.fit_resample(X, y)\n",
        "# Feature selection\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_smote_enn = selector.fit_transform(X_smote_enn, y_smote_enn)\n",
        "# Analyze the Target column distribution after SMOTE-ENN\n",
        "print(\"\\nTarget Class Distribution (After SMOTE-ENN):\")\n",
        "class_counts_smote_enn = pd.Series(y_smote_enn).value_counts()\n",
        "print(f\"Class 0: {class_counts_smote_enn[0]}\")\n",
        "print(f\"Class 1: {class_counts_smote_enn[1]}\")\n",
        "# Reshape data for Conv1D (samples, time steps, features)\n",
        "X_smote_enn_cnn = X_smote_enn[..., np.newaxis]  # Add a new axis for the feature dimension\n",
        "# Split data into training and testing sets\n",
        "X_train_cnn, X_test_cnn, y_train, y_test = train_test_split(X_smote_enn_cnn, y_smote_enn, test_size=0.2, random_state=42)\n",
        "X_train, X_test, _, _ = train_test_split(X_smote_enn, y_smote_enn, test_size=0.2, random_state=42)\n",
        "# Build the CNN model\n",
        "\"\"\" The code defines a Convolutional Neural Network (CNN) using Keras.\n",
        "It starts with a Conv1D layer (128 filters, kernel size 3) followed by MaxPooling1D to downsample.\n",
        "A second Conv1D layer (64 filters) and pooling layer are applied. The Flatten layer reshapes the output for dense layers.\n",
        "A Dense layer with 128 units and ReLU activation follows, then a Dropout layer (40% rate) helps prevent overfitting.\n",
        " The final Dense layer outputs a single value with a sigmoid activation for binary classification.\n",
        " The model is compiled with Adam optimizer, binary cross-entropy loss, and accuracy as the metric.\n",
        "This structure leverages convolutional layers for feature extraction, pooling for dimensionality reduction, and dense layers for decision-making.\"\"\"\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(128, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Train CNN model\n",
        "cnn_history = cnn_model.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test), epochs=150, batch_size=32, verbose=1)\n",
        "\n",
        "cnn_model.save(\"cnn_model.h5\")\n",
        "\n",
        "# Hyperparameter tuning for SVM\n",
        "\"\"\" The code performs hyperparameter tuning for a Support Vector Machine (SVM) using GridSearchCV.\n",
        "It initializes an SVM model with probability=True and a fixed random_state=42.\n",
        " The hyperparameters to be tuned are C (regularization parameter), kernel (either 'linear' or 'rbf'), and gamma (either 'scale' or 'auto').\n",
        " The grid search uses 5-fold cross-validation (cv=5) and accuracy as the scoring metric.\n",
        "  After fitting the model, the best estimator is retrieved with grid_svm.best_estimator_.\n",
        "This process helps find the optimal hyperparameters for improved model performance.\"\"\"\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "svm_params = {'C': [0.1, 1, 15], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
        "grid_svm = GridSearchCV(svm, svm_params, cv=5, scoring='accuracy')\n",
        "grid_svm.fit(X_train, y_train)\n",
        "svm_best = grid_svm.best_estimator_\n",
        "# Train Random Forest model with hyperparameter tuning\n",
        "\"\"\" The code performs hyperparameter tuning for a Random Forest classifier using GridSearchCV.\n",
        "It tunes the n_estimators (number of trees) and max_depth (maximum depth of trees) hyperparameters.\n",
        "The grid search uses 5-fold cross-validation (cv=5) and accuracy as the scoring metric.\n",
        "The best model is obtained via grid_rf.best_estimator_.\"\"\"\n",
        "rf_params = {'n_estimators': [100, 200], 'max_depth': [10, 20, None]}\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy')\n",
        "grid_rf.fit(X_train, y_train)\n",
        "rf_best = grid_rf.best_estimator_\n",
        "# Gradient Boosting Classifier\n",
        "\"\"\" The code defines and trains a Gradient Boosting Classifier model.\n",
        " It initializes the model with n_estimators=100 (number of boosting stages), learning_rate=0.1 (shrinkage factor to control model contribution per stage), and max_depth=3 (maximum depth of individual trees).\n",
        " The random_state=42 ensures reproducibility.\n",
        " After setting the parameters, the model is trained on the training data (X_train, y_train) using the fit() method.\n",
        "Gradient Boosting helps improve predictive performance by combining weak learners iteratively.\"\"\"\n",
        "gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "# Combine models using VotingClassifier\n",
        "\"\"\" The code combines multiple models into an ensemble using VotingClassifier.\n",
        "It includes a Support Vector Machine (svm_best), a Random Forest (rf_best), and a Gradient Boosting Model (gbm_model).\n",
        "The voting='soft' parameter specifies that predictions are made based on the weighted average of predicted probabilities, improving model accuracy.\"\"\"\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[('svm', svm_best), ('rf', rf_best), ('gbm', gbm_model)],\n",
        "    voting='soft'\n",
        ")\n",
        "# Train the ensemble model\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "# Evaluate the ensemble model\n",
        "ensemble_accuracy = ensemble_model.score(X_test, y_test)\n",
        "print(f\"\\nEnsemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
        "# Predictions\n",
        "cnn_pred = (cnn_model.predict(X_test_cnn) > 0.5).astype(\"int32\")\n",
        "svm_pred = svm_best.predict(X_test)\n",
        "rf_pred = rf_best.predict(X_test)\n",
        "gbm_pred = gbm_model.predict(X_test)\n",
        "ensemble_pred = ensemble_model.predict(X_test)\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, ensemble_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig(\"confusion_matrix_ensemble.png\")\n",
        "plt.show()\n",
        "# Classification Report\n",
        "report = classification_report(y_test, ensemble_pred, output_dict=True)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, ensemble_pred))\n",
        "# Save classification report\n",
        "with open(\"classification_report_ensemble.json\", \"w\") as f:\n",
        "    json.dump(report, f)\n",
        "    # ROC-AUC Curve\n",
        "\"\"\" The code generates and plots the ROC-AUC curve for the ensemble model's performance.\n",
        "It first calculates predicted probabilities (ensemble_pred_prob) for the test set, then computes the false positive rate (FPR) and true positive rate (TPR) using roc_curve.\n",
        " The AUC is calculated with auc().\n",
        " The ROC curve is plotted, showing the tradeoff between TPR and FPR, and the plot is saved as \"roc_curve_ensemble.png\".\n",
        "The curve helps evaluate the model's classification ability.\"\"\"\n",
        "# Fit the ensemble model first\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predicted probabilities for the test set\n",
        "ensemble_pred_prob = ensemble_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate FPR, TPR, and AUC for the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, ensemble_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(\"roc_curve_ensemble.png\")\n",
        "plt.show()\n",
        "# Explainability using SHAP\n",
        "\"\"\" The code uses SHAP to explain the Random Forest model (rf_best).\n",
        "It creates a TreeExplainer to compute SHAP values for the test set (X_test) and generates a summary plot to visualize feature importance and contributions to predictions.\"\"\"\n",
        "explainer = shap.TreeExplainer(rf_best)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values, X_test, feature_names=data.columns[:-1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f_UmTAw8d-Ay"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "snW9mi8ogjFm"
      },
      "outputs": [],
      "source": [
        "with open('svm_best.pkl', 'wb') as f:\n",
        "    pickle.dump(svm_best, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W25OZZP7gksE"
      },
      "outputs": [],
      "source": [
        "with open('rf_best.pkl', 'wb') as f:\n",
        "    pickle.dump(rf_best, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mvvGHYEvg3KO"
      },
      "outputs": [],
      "source": [
        "with open('ensemble_model.pkl', 'wb') as f:\n",
        "    pickle.dump(ensemble_model, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
